# **Behavioral Cloning** 

## Writeup Template

### You can use this file as a template for your writeup if you want to submit it as a markdown file, but feel free to use some other method and submit a pdf if you prefer.

---

**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./examples/image1.png "NVIDIA's PilotNet architecture"
[image2]: ./examples/image2.jpg "Center lane driving"

## Rubric Points
### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode
This project, Behavioral Cloning, the 3rd project of ND013: Self-driving Car Engineer Nanodegree Program consists of following files.
* train.py contains the execution script to import submodules and perform neural network model training.
* model.py contains the script to create and train the model.
* drive.py is for driving the car in autonomous mode.
* model.h5 contains a trained convolution neural network.
* writeup_report.md summarizes the results.

#### 2. Submission includes functional code
Using the simulator and <drive.py> provided by Udacity, the car can be driven autonomously around the track by executing the following command at Unix shell.
python drive.py model.h5

#### 3. Submission code is usable and readable
The <train.py> file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.
The <model.py> defines the neural network model implementation from the referenced paper.
The <drive.py> was exploited with almost no modification, except for print states for debugging and speed change.
The <genAugData.py> is a python code script for data preprocessing and augmentation.



### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed
The adopted model is referenced from NVIDIA's end-to-end learning network for self driving cars. The referenced papers are "End to End Learning for Self-Driving Cars" by Bojarski et al., arXiv Apr 2016, and "Explaining How a Deep Neural Netowrk Trained with End-to-End Learning Steers a Car" by Bojarski et al., arXiv Apr 2017.

#### 2. Attempts to reduce overfitting in the model
The model contains batch normalization layers to avoid overfitting. 
The model was trained and validated on preprocessed datasets after data augmentation.

#### 3. Model parameter tuning
An Adam optimizer was used with MSE (Mean Square Error) loss function and learning rate of 0.005.

#### 4. Appropriate training data
Training data was generated by carefully driving the car in simulation program to keep the lane and road without falling off. The dataset was made from driving 3 laps clockwise and 3 laps counter-clockwise.


### Model Architecture and Training Strategy

#### 1. Solution Design Approach
Based on NVIDIA's PilotNet CNN model, the model was designed in <model.py>, which was proven to operate well for end-to-end learning of autonomous driving cars.
The training data set was made by driving the simulated car carefully not to get out of the road.
The entire training process is operated by <train.py>, calling the model defined in <model.py> first and then executing data augmentation procedure by calling <genAugData.py> to generate training and validation data. After that, <train.py> calls the function <fit_generator> to train the model.

#### 2. Final Model Architecture
The final model architecture consisted of following layers using Keras API with Tensorflow, based on NVIDIA's PilotNet architecture.
The 1st layer is a lambda layer cropping the original image.
The 2nd layer is a lambda layer normalizing the cropped image, making the center average to 0.
Then, five sequential sets of convolutional layer, ReLU activation layer, and batch normalization layers are followed.
A flatten layer is followed after that, and four sequential fully connected layers are followed before the last output layer.
At last, the model is inserted to <multi_gpu_model> function to accelerate training process using a local GPU machine in the laboratory consisting of a NVIDIA Titan V and two NVIDIA Titan XP GPUs.
Finally, the model is compiled using loss function of MSE and Adam optimizer.
The overall network model structure is visualized at [image1].
![alt text][image1]

#### 3. Creation of the Training Set & Training Process
To make training data with good quality, the simulator car was driven carefully not to get out of the road even though the speed is rather slow. The example image of center lane driving is shown at [image2] below.
![alt text][image2]

With center lane driving on the provided track 1, two datasets of driving clockwise 3 laps were recorded, and then another two datasets of driving counter-clockwise 3 laps were recorded.


In <genAugData.py>, for data augmentation, the image frame datasets collected from driving simulator are first read by the function <read_img>. Then the read image is preprocessed for data augmentation including the following functions.
<flipHorImg>: horizontally flipping training data images, regarding reversed direction driving
<blurGauImg>: applying Gaussian blurring to training data images
<modBrigImg>: changing brighteness of training data images
<addShadImg>: adding random shadows to training data images
<AugDataImg>: performing data augmentation to a training data image, by applying a series of transformation functions described above
<genImgs>: generating training image datasets by applying data augmentation functions above to the original image datasets and yielding batches with modified steering angles


In <train.py>, the entire training process is performed. Firstly, the datasets are loaded by designating the dataset paths and .csv files and parsing the csv files into header colomns. The parsing process is performed by <load_csv> file, which adopts <pandas> library.
Then, the image frames are concatenated, again by using <concat> function in <pandas> library.
After that, hyperparameters including batch size and batch size divider is set, and the model defined in <model.py> is loaded. The train datasets and the validation datasets are generated by <genImgs> function defined in <genAugData.py>.
Finally, the training process is executed by <fit_generator> function provided by Keras API. After finishing the training process, the model except for the last two layers, which generates "Not Serializable JSON" error due to the use of <multi_gpu_model> function, is saved by <save> function offered by Keras API.

